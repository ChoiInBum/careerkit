"""
ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬ ëª¨ë“ˆ
ChromaDB ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ë° ê´€ë¦¬
"""
from pathlib import Path
from typing import List, Dict, Optional

# ì§€ì—° ë¡œë”©ì„ ìœ„í•´ ëª¨ë“ˆ ë ˆë²¨ì—ì„œëŠ” importí•˜ì§€ ì•ŠìŒ
CHROMADB_AVAILABLE = None
chromadb = None
SentenceTransformer = None

VECTOR_STORE = None
EMBEDDING_MODEL = None
_INITIALIZED = False


def _check_dependencies():
    """ì˜ì¡´ì„± í™•ì¸ ë° ì§€ì—° ë¡œë”©"""
    global CHROMADB_AVAILABLE, chromadb, SentenceTransformer
    
    if CHROMADB_AVAILABLE is not None:
        return CHROMADB_AVAILABLE
    
    try:
        import chromadb
        from sentence_transformers import SentenceTransformer
        CHROMADB_AVAILABLE = True
        return True
    except ImportError:
        CHROMADB_AVAILABLE = False
        chromadb = None
        SentenceTransformer = None
        return False


def initialize_vector_store_components(force_reload: bool = False):
    """ë²¡í„° ìŠ¤í† ì–´ ë° ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
    global VECTOR_STORE, EMBEDDING_MODEL, _INITIALIZED
    
    try:
        if not _check_dependencies():
            print("âš ï¸  chromadb ë˜ëŠ” sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ì‹¤í–‰: pip install chromadb sentence-transformers")
            _INITIALIZED = False
            return False
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        print("ğŸ“¦ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
        EMBEDDING_MODEL = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: paraphrase-multilingual-MiniLM-L12-v2")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
        project_root = Path(__file__).parent.parent
        chroma_db_path = project_root / "chroma_db"
        print(f"ğŸ“ ChromaDB ê²½ë¡œ: {chroma_db_path}")
        
        chroma_client = chromadb.PersistentClient(path=str(chroma_db_path))
        
        # ê°•ì œ ì¬ë¡œë“œì¸ ê²½ìš° ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
        if force_reload:
            try:
                chroma_client.delete_collection(name="saramin_jobs")
                print("ğŸ”„ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ")
            except Exception as e:
                print(f"â„¹ï¸  ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì‹œë„ (ì—†ì„ ìˆ˜ ìˆìŒ): {e}")
        
        VECTOR_STORE = chroma_client.get_or_create_collection(
            name="saramin_jobs",
            metadata={"hnsw:space": "cosine"}
        )
        print("âœ… ChromaDB ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ì´ˆê¸°í™” ìƒíƒœ í™•ì¸
        if VECTOR_STORE is not None:
            try:
                count = VECTOR_STORE.count()
                print(f"ğŸ“Š í˜„ì¬ ë²¡í„° ìŠ¤í† ì–´ ë¬¸ì„œ ìˆ˜: {count}ê°œ")
            except AttributeError as e:
                print(f"âš ï¸  VECTOR_STOREì— count() ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤: {e}")
            except Exception as e:
                print(f"âš ï¸  ë²¡í„° ìŠ¤í† ì–´ ì¹´ìš´íŠ¸ í™•ì¸ ì‹¤íŒ¨: {e}")
        else:
            print("âš ï¸  VECTOR_STOREê°€ Noneì…ë‹ˆë‹¤.")
        
        _INITIALIZED = True
        return True
        
    except Exception as e:
        print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        _INITIALIZED = False
        VECTOR_STORE = None
        EMBEDDING_MODEL = None
        return False


def is_vector_store_initialized() -> bool:
    """ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ìƒíƒœ í™•ì¸"""
    global VECTOR_STORE, EMBEDDING_MODEL, _INITIALIZED
    result = _INITIALIZED and VECTOR_STORE is not None and EMBEDDING_MODEL is not None
    if not result:
        print(f"ğŸ” is_vector_store_initialized() ì²´í¬:")
        print(f"   - _INITIALIZED: {_INITIALIZED}")
        print(f"   - VECTOR_STORE is None: {VECTOR_STORE is None}")
        print(f"   - EMBEDDING_MODEL is None: {EMBEDDING_MODEL is None}")
    return result


def initialize_vector_store(
    jobs: List[Dict], 
    chunk_size: int = 0, 
    force_reload: bool = False,
    window_size: int = 500,
    stride: int = 200
) -> bool:
    """ì±„ìš©ê³µê³ ë¥¼ window + stride ë°©ì‹ìœ¼ë¡œ ì²­í‚¹í•˜ì—¬ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥
    
    Args:
        jobs: ì±„ìš©ê³µê³  ë¦¬ìŠ¤íŠ¸
        chunk_size: ë ˆê±°ì‹œ íŒŒë¼ë¯¸í„° (0ì´ë©´ window+stride ë°©ì‹ ì‚¬ìš©, í˜¸í™˜ì„± ìœ ì§€)
        force_reload: ê¸°ì¡´ ë°ì´í„° ê°•ì œ ì¬ë¡œë“œ ì—¬ë¶€
        window_size: ì²­í¬ í¬ê¸° (ê¸°ë³¸ê°’: 500ì)
        stride: ì˜¤ë²„ë© í¬ê¸° (ê¸°ë³¸ê°’: 200ì, ì‹¤ì œ ì´ë™ ê±°ë¦¬ = window_size - stride = 300ì)
    
    Note:
        - window_size: ê° ì²­í¬ì˜ í¬ê¸° (500ì)
        - stride: ì˜¤ë²„ë© í¬ê¸° (200ì) - ì´ì „ ì²­í¬ì™€ ê²¹ì¹˜ëŠ” ë¶€ë¶„
        - ì‹¤ì œ ì´ë™ ê±°ë¦¬: window_size - stride = 300ì
        - ì˜ˆ: 1000ì í…ìŠ¤íŠ¸ â†’ ì²­í¬1(0-500), ì²­í¬2(300-800), ì²­í¬3(600-1000)
    """
    if not is_vector_store_initialized():
        print("âš ï¸  ë²¡í„° ìŠ¤í† ì–´ ë˜ëŠ” ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print(f"   - _INITIALIZED: {_INITIALIZED}")
        print(f"   - VECTOR_STORE: {VECTOR_STORE is not None}")
        print(f"   - EMBEDDING_MODEL: {EMBEDDING_MODEL is not None}")
        return False
    
    # force_reloadê°€ Falseì¸ ê²½ìš°ì—ë§Œ ê¸°ì¡´ ë°ì´í„° í™•ì¸ (ì±„ìš©ê³µê³ ë§Œ í™•ì¸)
    if not force_reload and VECTOR_STORE is not None:
        try:
            # ì „ì²´ ë¬¸ì„œ ìˆ˜ í™•ì¸
            total_count = VECTOR_STORE.count()
            if total_count > 0:
                # ì±„ìš©ê³µê³  ë¬¸ì„œë§Œ ì¹´ìš´íŠ¸ (typeì´ 'resume'ì´ ì•„ë‹Œ ê²ƒë“¤)
                # ChromaDBì—ì„œ ë©”íƒ€ë°ì´í„°ë¡œ í•„í„°ë§í•˜ì—¬ ì±„ìš©ê³µê³ ë§Œ í™•ì¸
                try:
                    # ìƒ˜í”Œ ì¡°íšŒë¡œ ì±„ìš©ê³µê³ ê°€ ìˆëŠ”ì§€ í™•ì¸
                    sample_results = VECTOR_STORE.get(limit=100)
                    if sample_results and 'metadatas' in sample_results:
                        job_count = sum(1 for meta in sample_results['metadatas'] 
                                      if meta.get('type') != 'resume')
                        if job_count > 0:
                            print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ì— ì´ë¯¸ ì±„ìš©ê³µê³  ë°ì´í„°ê°€ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ì „ì²´ ë¬¸ì„œ: {total_count}ê°œ)")
                            return True
                except Exception:
                    # í•„í„°ë§ ì‹¤íŒ¨ ì‹œ ì „ì²´ ì¹´ìš´íŠ¸ë¡œ íŒë‹¨
                    if total_count > 10:  # ì´ë ¥ì„œëŠ” ë³´í†µ 1ê°œ ì •ë„ì´ë¯€ë¡œ, 10ê°œ ì´ìƒì´ë©´ ì±„ìš©ê³µê³ ê°€ ìˆë‹¤ê³  íŒë‹¨
                        print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ì— ì´ë¯¸ ë°ì´í„°ê°€ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ë¬¸ì„œ ìˆ˜: {total_count}ê°œ)")
                        return True
        except (AttributeError, Exception) as e:
            print(f"âš ï¸  ê¸°ì¡´ ë°ì´í„° í™•ì¸ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰): {e}")
    
    if not jobs:
        print("âš ï¸  íŒŒì‹±ëœ ì±„ìš©ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    # ê° ê³µê³ ë¥¼ ì²­í‚¹í•˜ì—¬ ë²¡í„°í™”í•˜ì—¬ ì €ì¥
    documents = []
    metadatas = []
    ids = []
    
    for idx, job in enumerate(jobs):
        # ê²€ìƒ‰ì— ì‚¬ìš©í•  ì „ì²´ í…ìŠ¤íŠ¸ êµ¬ì„±
        full_text = f"""
ì œëª©: {job.get('title', '')}
íšŒì‚¬: {job.get('company', '')}
ì§€ì—­: {job.get('location', '')}
ê³ ìš©í˜•íƒœ: {job.get('job_type', '')}
ê¸°ì—…ê·œëª¨: {job.get('company_size', '')}
ì‚°ì—…êµ°: {job.get('industry', '')}
ì£¼ìš”ì—…ë¬´: {job.get('work', '')}
ìê²©ìš”ê±´: {job.get('requirements', '')}
ê·¼ë¬´ì¡°ê±´: {job.get('conditions', '')}
ê¸‰ì—¬ë°ë³µë¦¬í›„ìƒ: {job.get('benefits', '')}
ì „ì²´ë‚´ìš©: {job.get('full_text', '')}
""".strip()
        
        # window + stride ë°©ì‹ìœ¼ë¡œ ì²­í‚¹
        text_length = len(full_text)
        
        # strideê°€ window_sizeë³´ë‹¤ í¬ê±°ë‚˜ ê°™ìœ¼ë©´ ì˜¤ë¥˜
        if stride >= window_size:
            print(f"âš ï¸  stride({stride})ê°€ window_size({window_size})ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ìŠµë‹ˆë‹¤. strideë¥¼ {window_size // 2}ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.")
            stride = window_size // 2
        
        # ì‹¤ì œ ì´ë™ ê±°ë¦¬ (overlap = stride, step = window_size - stride)
        step_size = window_size - stride
        
        if text_length <= window_size:
            # ê³µê³ ê°€ window_sizeë³´ë‹¤ ì‘ìœ¼ë©´ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ì €ì¥
            if full_text.strip():
                documents.append(full_text)
                metadatas.append({
                    "title": job.get('title', ''),
                    "company": job.get('company', ''),
                    "location": job.get('location', ''),
                    "job_type": job.get('job_type', ''),
                    "company_size": job.get('company_size', ''),
                    "industry": job.get('industry', ''),
                    "url": job.get('url', ''),
                    "job_id": idx,
                    "chunk_index": 0,
                    "total_chunks": 1,
                    "chunk_start": 0,
                    "chunk_end": text_length,
                    "chunk_length": text_length,
                    "window_size": window_size,
                    "stride": stride,
                    "full_text": full_text,  # ì „ì²´ ê³µê³  í…ìŠ¤íŠ¸ ì €ì¥
                    "type": "job"
                })
                ids.append(f"job_{idx}_chunk_0")
        else:
            # window + stride ë°©ì‹ìœ¼ë¡œ ì²­í‚¹
            chunks = []
            start_idx = 0
            chunk_idx = 0
            
            while start_idx < text_length:
                end_idx = min(start_idx + window_size, text_length)
                chunk_text = full_text[start_idx:end_idx]
                
                # ë¹ˆ ì²­í¬ëŠ” ì œì™¸
                if chunk_text.strip():
                    chunks.append({
                        "text": chunk_text,
                        "start": start_idx,
                        "end": end_idx,
                        "index": chunk_idx
                    })
                    chunk_idx += 1
                
                # ë‹¤ìŒ ì‹œì‘ ìœ„ì¹˜ë¡œ ì´ë™ (step_sizeë§Œí¼ ì´ë™, overlap = stride)
                start_idx += step_size
                
                # ë§ˆì§€ë§‰ ë¶€ë¶„ ì²˜ë¦¬: ë‚¨ì€ í…ìŠ¤íŠ¸ê°€ step_sizeë³´ë‹¤ ì‘ìœ¼ë©´ ë§ˆì§€ë§‰ ì²­í¬ë¡œ ì¶”ê°€
                if start_idx < text_length and start_idx + step_size >= text_length:
                    # ë§ˆì§€ë§‰ ë¶€ë¶„ì´ ë‚¨ì•„ìˆê³ , ì•„ì§ ì¶”ê°€í•˜ì§€ ì•Šì•˜ìœ¼ë©´ ì¶”ê°€
                    if end_idx < text_length:
                        last_chunk = full_text[start_idx:]
                        if last_chunk.strip() and len(last_chunk) >= stride:  # ìµœì†Œ stride í¬ê¸°ëŠ” ë˜ì–´ì•¼ ì˜ë¯¸ ìˆìŒ
                            chunks.append({
                                "text": last_chunk,
                                "start": start_idx,
                                "end": text_length,
                                "index": chunk_idx
                            })
                    break
            
            # ì²­í¬ ì €ì¥
            total_chunks = len(chunks)
            for chunk_info in chunks:
                documents.append(chunk_info["text"])
                metadatas.append({
                    "title": job.get('title', ''),
                    "company": job.get('company', ''),
                    "location": job.get('location', ''),
                    "job_type": job.get('job_type', ''),
                    "company_size": job.get('company_size', ''),
                    "industry": job.get('industry', ''),
                    "url": job.get('url', ''),
                    "job_id": idx,
                    "chunk_index": chunk_info["index"],
                    "total_chunks": total_chunks,
                    "chunk_start": chunk_info["start"],
                    "chunk_end": chunk_info["end"],
                    "chunk_length": len(chunk_info["text"]),
                    "window_size": window_size,
                    "stride": stride,
                    "full_text": full_text,  # ì „ì²´ ê³µê³  í…ìŠ¤íŠ¸ ì €ì¥ (ëª¨ë“  ì²­í¬ì— ë™ì¼í•˜ê²Œ ì €ì¥)
                    "type": "job"
                })
                ids.append(f"job_{idx}_chunk_{chunk_info['index']}")
    
    # ì„ë² ë”© ìƒì„± ë° ì €ì¥
    print(f"ğŸ“Š {len(documents)}ê°œì˜ ì²­í¬ ì„ë² ë”© ìƒì„± ì¤‘... (ì›ë³¸ ê³µê³ : {len(jobs)}ê°œ)")
    embeddings = EMBEDDING_MODEL.encode(documents, show_progress_bar=True)
    
    # ChromaDBì— ì €ì¥
    VECTOR_STORE.add(
        embeddings=embeddings.tolist(),
        documents=documents,
        metadatas=metadatas,
        ids=ids
    )
    
    print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ì— {len(documents)}ê°œì˜ ì²­í¬ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤. (ì›ë³¸ ê³µê³ : {len(jobs)}ê°œ)")
    return True


def add_resume_to_vector_store(resume: Dict, session_id: str) -> bool:
    """ì´ë ¥ì„œë¥¼ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥"""
    if not VECTOR_STORE or not EMBEDDING_MODEL:
        print("âš ï¸  ë²¡í„° ìŠ¤í† ì–´ ë˜ëŠ” ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    # ì´ë ¥ì„œ í…ìŠ¤íŠ¸ êµ¬ì„±
    resume_text = f"""
ì´ë¦„: {resume.get('name', '')}
í•™ë ¥: {resume.get('education', [])}
ê²½ë ¥: {resume.get('experience', [])}
ê¸°ìˆ  ìŠ¤íƒ: {', '.join(resume.get('skills', []))}
ìê²©ì¦: {', '.join([c.get('name', '') for c in resume.get('certificates', [])])}
í”„ë¡œì íŠ¸: {', '.join([p.get('name', '') for p in resume.get('projects', [])])}
ì „ì²´ ë‚´ìš©: {resume.get('full_text', '')}
""".strip()
    
    if not resume_text.strip():
        return False
    
    # ì„ë² ë”© ìƒì„±
    embedding = EMBEDDING_MODEL.encode([resume_text])
    
    # ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥
    VECTOR_STORE.add(
        embeddings=embedding.tolist(),
        documents=[resume_text],
        metadatas=[{
            "type": "resume",
            "session_id": session_id,
            "name": resume.get('name', ''),
            "resume_id": session_id
        }],
        ids=[f"resume_{session_id}"]
    )
    
    print(f"âœ… ì´ë ¥ì„œë¥¼ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. (session_id: {session_id})")
    return True


def search_vector_store(keywords: List[str], top_k: int = 10) -> List[Dict]:
    """ë²¡í„° ìŠ¤í† ì–´ì—ì„œ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰"""
    # ì „ì—­ ë³€ìˆ˜ ì°¸ì¡° (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ global ì„ ì–¸ í•„ìš”)
    global VECTOR_STORE, EMBEDDING_MODEL
    
    print(f"\n  [search_vector_store] ì‹œì‘: keywords={len(keywords)}ê°œ, top_k={top_k}")
    print(f"     - í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸: {keywords[:5]}{'...' if len(keywords) > 5 else ''}")
    
    if not VECTOR_STORE or not EMBEDDING_MODEL:
        print("  âŒ [search_vector_store] VECTOR_STORE ë˜ëŠ” EMBEDDING_MODELì´ Noneì…ë‹ˆë‹¤.")
        print(f"     - VECTOR_STORE: {VECTOR_STORE is not None}")
        print(f"     - EMBEDDING_MODEL: {EMBEDDING_MODEL is not None}")
        return []
    
    # ë²¡í„° ìŠ¤í† ì–´ì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
    if VECTOR_STORE is None:
        print("  âŒ [search_vector_store] VECTOR_STOREê°€ Noneì…ë‹ˆë‹¤.")
        return []
    
    try:
        doc_count = VECTOR_STORE.count()
        print(f"  ğŸ“Š [search_vector_store] ë²¡í„° ìŠ¤í† ì–´ ë¬¸ì„œ ìˆ˜: {doc_count}ê°œ")
        if doc_count == 0:
            print("  âŒ [search_vector_store] ë²¡í„° ìŠ¤í† ì–´ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì´ˆê¸°í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return []
    except AttributeError as e:
        print(f"  âŒ [search_vector_store] VECTOR_STOREì— count() ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤: {e}")
        return []
    except Exception as e:
        print(f"  âŒ [search_vector_store] ë²¡í„° ìŠ¤í† ì–´ ì¹´ìš´íŠ¸ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return []
    
    # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± (í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©)
    if not keywords:
        print("  âŒ [search_vector_store] í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return []
    
    query_text = " ".join(keywords)
    print(f"  ğŸ” [search_vector_store] ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸: {query_text[:200]}...")
    print(f"     - í‚¤ì›Œë“œ ê°œìˆ˜: {len(keywords)}")
    print(f"     - ì¿¼ë¦¬ ê¸¸ì´: {len(query_text)} ë¬¸ì")
    
    try:
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        print(f"  â³ [search_vector_store] ì„ë² ë”© ìƒì„± ì¤‘...")
        query_embedding = EMBEDDING_MODEL.encode([query_text])
        print(f"     - ì„ë² ë”© ì°¨ì›: {query_embedding.shape}")
        
        # ë²¡í„° ê²€ìƒ‰ (ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ë” ë§ì´ ê°€ì ¸ì˜¤ê¸°)
        n_results = min(top_k * 2, doc_count)  # ë¬¸ì„œ ìˆ˜ë³´ë‹¤ ë§ì´ ìš”ì²­í•˜ì§€ ì•Šë„ë¡
        print(f"  ğŸ” [search_vector_store] ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰: n_results={n_results}")
        
        # query_embeddingì€ (1, 384) í˜•íƒœì´ë¯€ë¡œ, tolist()í•˜ë©´ [[...]] í˜•íƒœê°€ ë¨
        # ChromaDBëŠ” query_embeddingsì— ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        embedding_list = query_embedding.tolist()
        print(f"     - ì„ë² ë”© ë¦¬ìŠ¤íŠ¸ í˜•íƒœ: {len(embedding_list)}ê°œ ë¦¬ìŠ¤íŠ¸, ê° {len(embedding_list[0]) if embedding_list else 0}ì°¨ì›")
        
        results = VECTOR_STORE.query(
            query_embeddings=embedding_list,  # ì´ë¯¸ [[...]] í˜•íƒœì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬
            n_results=n_results
        )
        
        raw_result_count = len(results.get('ids', [[]])[0]) if results.get('ids') else 0
        print(f"  ğŸ“Š [search_vector_store] ë²¡í„° ê²€ìƒ‰ ì›ì‹œ ê²°ê³¼: {raw_result_count}ê°œ")
        
        # ê²°ê³¼ ë³€í™˜ (ì¤‘ë³µ ì œê±° ì—†ì´ ëª¨ë“  chunk ë°˜í™˜)
        search_results = []
        resume_count = 0
        
        if results.get('ids') and len(results['ids'][0]) > 0:
            for i in range(len(results['ids'][0])):
                metadata = results['metadatas'][0][i]
                doc_type = metadata.get('type', 'unknown')
                
                # ì´ë ¥ì„œëŠ” ì œì™¸í•˜ê³  ì±„ìš©ê³µê³ ë§Œ í¬í•¨
                if doc_type == 'resume':
                    resume_count += 1
                    continue
                
                # ëª¨ë“  chunkë¥¼ ë°˜í™˜ (ì¤‘ë³µ ì œê±° ì—†ìŒ)
                search_results.append({
                    "id": results['ids'][0][i],
                    "document": results['documents'][0][i],
                    "metadata": metadata,
                    "distance": results['distances'][0][i] if 'distances' in results and results['distances'] else 0.0
                })
        
        print(f"  âœ… [search_vector_store] ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ chunk (ì¤‘ë³µ ì œê±° ì—†ìŒ)")
        if resume_count > 0:
            print(f"     - ì œì™¸ëœ ì´ë ¥ì„œ: {resume_count}ê°œ")
        
        return search_results
        
    except Exception as e:
        print(f"  âŒ [search_vector_store] ë²¡í„° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return []

