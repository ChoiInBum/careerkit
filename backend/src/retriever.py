"""
Retriever ëª¨ë“ˆ: ì´ë ¥ì„œì™€ ì±—ë´‡ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬ ê³µê³  ì¶”ì¶œ
"""
from typing import List, Dict
from difflib import SequenceMatcher


def extract_experience_keyword(resume: Dict) -> str:
    """ì´ë ¥ì„œì—ì„œ ê²½ë ¥ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ í‚¤ì›Œë“œ ë°˜í™˜ (ì‹ ì…/ê²½ë ¥)
    
    Args:
        resume: ì´ë ¥ì„œ ë”•ì…”ë„ˆë¦¬
    
    Returns:
        "ì‹ ì…" ë˜ëŠ” "ê²½ë ¥"
    """
    if not resume:
        return "ì‹ ì…"
    
    experience = resume.get('experience', [])
    
    # ê²½ë ¥ ì •ë³´ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ì‹ ì…
    if not experience or len(experience) == 0:
        return "ì‹ ì…"
    
    # ê²½ë ¥ ì •ë³´ê°€ ìˆìœ¼ë©´ ê²½ë ¥
    return "ê²½ë ¥"


def build_weighted_query(resume: Dict, slots: Dict) -> tuple[List[str], Dict[str, float]]:
    """ì±—ë´‡ ì •ë³´ì™€ ì´ë ¥ì„œ ê²½ë ¥ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± (ìµœëŒ€ 5ê°œ)
    
    Returns:
        tuple: (query_keywords, keyword_weights)
    """
    query_keywords = []
    keyword_weights = {}  # í‚¤ì›Œë“œë³„ ì¤‘ìš”ë„
    
    # ì±—ë´‡ ì •ë³´ ì‚¬ìš©
    if slots:
        desired_job = slots.get('desired_job', '').strip()
        if desired_job:
            query_keywords.append(desired_job)
            keyword_weights[desired_job] = 3.0
        
        location = slots.get('location', '').strip()
        if location:
            query_keywords.append(location)
            keyword_weights[location] = 2.5
        
        job_type = slots.get('job_type', '').strip()
        if job_type:
            query_keywords.append(job_type)
            keyword_weights[job_type] = 2.0
        
        industry = slots.get('industry', '').strip()
        if industry:
            query_keywords.append(industry)
            keyword_weights[industry] = 2.0
        
        company_size = slots.get('company_size', '').strip()
        if company_size:
            query_keywords.append(company_size)
            keyword_weights[company_size] = 1.5
    
    # ì´ë ¥ì„œì—ì„œ ê²½ë ¥ í‚¤ì›Œë“œ ì¶”ì¶œ
    experience_keyword = extract_experience_keyword(resume)
    if experience_keyword:
        query_keywords.append(experience_keyword)
        keyword_weights[experience_keyword] = 2.5  # ê²½ë ¥ ì •ë³´ëŠ” ë†’ì€ ê°€ì¤‘ì¹˜
    
    # ìµœëŒ€ 5ê°œë¡œ ì œí•œ (ê²½ë ¥ í‚¤ì›Œë“œ í¬í•¨)
    query_keywords = query_keywords[:5]
    keyword_weights = {k: v for k, v in keyword_weights.items() if k in query_keywords}
    
    return query_keywords, keyword_weights


def calculate_string_similarity(str1: str, str2: str) -> float:
    """ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚° (0.0 ~ 1.0)"""
    return SequenceMatcher(None, str1.lower(), str2.lower()).ratio()


def find_similar_keywords(keyword: str, text: str, threshold: float = 0.6) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ìœ ì‚¬í•œ í‚¤ì›Œë“œë¥¼ ì°¾ê¸° (ìœ ì‚¬ ì˜ë¯¸ê¹Œì§€ ì¸ì •)
    
    Args:
        keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ
        text: ê²€ìƒ‰ ëŒ€ìƒ í…ìŠ¤íŠ¸
        threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0 ~ 1.0)
    
    Returns:
        ìœ ì‚¬í•œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    keyword_lower = keyword.lower()
    text_lower = text.lower()
    similar_keywords = []
    
    # ê²½ë ¥ ê´€ë ¨ í‚¤ì›Œë“œì˜ ìœ ì‚¬ ì˜ë¯¸ ë§¤í•‘
    experience_synonyms = {
        "ì‹ ì…": ["ì‹ ì…ì‚¬ì›", "ì‹ ì… ê°œë°œì", "ì‹ ì…ì", "ì£¼ë‹ˆì–´", "junior", "newbie", 
                "ì‹ ì… ì§€ì› ê°€ëŠ¥", "ì‹ ì… ê°€ëŠ¥", "ì‹ ì… í™˜ì˜", "ì‹ ì… ì±„ìš©", "ì‹ ì… ëª¨ì§‘",
                "ê²½ë ¥ ë¬´ê´€", "ê²½ë ¥ ì œí•œ ì—†ìŒ", "ì‹ ì…ë„ ê°€ëŠ¥", "ì‹ ì…ë„ í™˜ì˜"],
        "ê²½ë ¥": ["ê²½ë ¥ì‚¬ì›", "ê²½ë ¥ ê°œë°œì", "ê²½ë ¥ì", "ì‹œë‹ˆì–´", "senior", "ê²½ë ¥ ì±„ìš©",
                "ê²½ë ¥ ëª¨ì§‘", "ê²½ë ¥ ìš°ëŒ€", "ê²½ë ¥ í•„ìˆ˜", "ê²½ë ¥ 3ë…„", "ê²½ë ¥ 5ë…„", 
                "ê²½ë ¥ 7ë…„", "ê²½ë ¥ 10ë…„", "ê²½ë ¥ì§", "ê²½ë ¥ ì¸ì¬"]
    }
    
    # 1. ì •í™•í•œ ë§¤ì¹­ (ê³µë°± ì œê±° í›„)
    keyword_no_space = keyword_lower.replace(' ', '')
    if keyword_lower in text_lower or keyword_no_space in text_lower.replace(' ', ''):
        return [keyword]  # ì •í™•í•œ ë§¤ì¹­ì´ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
    
    # 2. ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ (í‚¤ì›Œë“œê°€ í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€)
    if keyword_lower in text_lower:
        return [keyword]
    
    # 3. ê²½ë ¥ ê´€ë ¨ í‚¤ì›Œë“œì˜ ê²½ìš° ìœ ì‚¬ ì˜ë¯¸ í™•ì¸
    if keyword in experience_synonyms:
        synonyms = experience_synonyms[keyword]
        for synonym in synonyms:
            if synonym.lower() in text_lower:
                return [keyword]  # ìœ ì‚¬ ì˜ë¯¸ ë°œê²¬ ì‹œ ë§¤ì¹­ìœ¼ë¡œ ì¸ì •
    
    # 4. ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ìœ ì‚¬ë„ ê³„ì‚°
    text_words = text_lower.split()
    keyword_words = keyword_lower.split()
    
    # í‚¤ì›Œë“œì˜ ê° ë‹¨ì–´ê°€ í…ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸
    matched_words = []
    for kw_word in keyword_words:
        for text_word in text_words:
            similarity = calculate_string_similarity(kw_word, text_word)
            if similarity >= threshold:
                matched_words.append(text_word)
                break
    
    # í‚¤ì›Œë“œì˜ ëª¨ë“  ë‹¨ì–´ê°€ ë§¤ì¹­ë˜ë©´ ìœ ì‚¬ í‚¤ì›Œë“œë¡œ ì¸ì •
    if len(matched_words) >= len(keyword_words) * 0.7:  # 70% ì´ìƒ ë§¤ì¹­
        return [keyword]
    
    # 5. í…ìŠ¤íŠ¸ì˜ ê° ë‹¨ì–´ì™€ í‚¤ì›Œë“œì˜ ìœ ì‚¬ë„ ê³„ì‚°
    for text_word in text_words:
        if len(text_word) < 2:  # ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ëŠ” ì œì™¸
            continue
        similarity = calculate_string_similarity(keyword_lower, text_word)
        if similarity >= threshold:
            similar_keywords.append(text_word)
    
    return similar_keywords if similar_keywords else []


def calculate_weighted_score(result: Dict, keyword_weights: Dict[str, float], query_keywords: List[str]) -> tuple:
    """ë²¡í„° ìœ ì‚¬ë„ì™€ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ë¥¼ ê²°í•©í•œ ìµœì¢… ì ìˆ˜ ê³„ì‚° (ìœ ì‚¬ í‚¤ì›Œë“œ ë§¤ì¹­ í¬í•¨)
    
    Args:
        result: search_vector_storeì—ì„œ ë°˜í™˜ëœ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        keyword_weights: í‚¤ì›Œë“œë³„ ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬
        query_keywords: ê²€ìƒ‰ ì¿¼ë¦¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        tuple: (final_score, matched_keywords)
    """
    # distanceë¥¼ ì ìˆ˜ë¡œ ë³€í™˜ (distanceëŠ” ì‘ì„ìˆ˜ë¡ ìœ ì‚¬ë„ê°€ ë†’ìŒ)
    # cosine distanceëŠ” 0~2 ë²”ìœ„ì´ë¯€ë¡œ, 1 - (distance / 2)ë¡œ ë³€í™˜í•˜ì—¬ 0~1 ë²”ìœ„ì˜ ì ìˆ˜ë¡œ ë§Œë“¦
    distance = result.get('distance', 1.0)
    vector_score = max(0.0, 1.0 - (distance / 2.0))  # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
    
    # ê³µê³  í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
    # metadataì˜ full_textë¥¼ ìš°ì„  ì‚¬ìš© (ì „ì²´ ê³µê³  í…ìŠ¤íŠ¸)
    metadata = result.get('metadata', {})
    
    # 1. metadataì—ì„œ full_text ê°€ì ¸ì˜¤ê¸° (ì „ì²´ ê³µê³  í…ìŠ¤íŠ¸)
    full_text = metadata.get('full_text', '')
    
    # 2. full_textê°€ ì—†ìœ¼ë©´ êµ¬ì¡°í™”ëœ ì •ë³´ë¡œ êµ¬ì„± (í•˜ìœ„ í˜¸í™˜ì„±)
    if not full_text:
        # document í…ìŠ¤íŠ¸ (ì²­í¬) - í•˜ìœ„ í˜¸í™˜ì„±
        job_text = result.get('document', '')
        
        # metadataì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´
        metadata_text_parts = [
            metadata.get('title', ''),
            metadata.get('company', ''),
            metadata.get('location', ''),
            metadata.get('job_type', ''),
            metadata.get('company_size', ''),
            metadata.get('industry', ''),
        ]
        metadata_text = ' '.join([str(part) for part in metadata_text_parts if part])
        
        # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•© (ì²­í¬ + ë©”íƒ€ë°ì´í„°)
        full_text = f"{job_text} {metadata_text}"
    
    full_text_lower = full_text.lower()
    
    keyword_bonus = 0.0
    matched_keywords = []
    matched_details = {}  # í‚¤ì›Œë“œë³„ ë§¤ì¹­ ìƒì„¸ ì •ë³´
    
    for keyword in query_keywords:
        keyword_lower = keyword.lower()
        weight = keyword_weights.get(keyword, 1.0)
        match_found = False
        match_type = None
        
        # 1. ì •í™•í•œ ë§¤ì¹­ (ê°€ì¥ ë†’ì€ ì ìˆ˜)
        if keyword_lower in full_text_lower:
            keyword_bonus += weight * 0.15  # ì •í™•í•œ ë§¤ì¹­ì€ ë” ë†’ì€ ë³´ë„ˆìŠ¤
            matched_keywords.append(keyword)
            match_found = True
            match_type = "ì •í™•"
        
        # 2. ê³µë°± ì œê±° í›„ ë§¤ì¹­
        elif keyword_lower.replace(' ', '') in full_text_lower.replace(' ', ''):
            keyword_bonus += weight * 0.12
            if keyword not in matched_keywords:
                matched_keywords.append(keyword)
            match_found = True
            match_type = "ê³µë°±ì œê±°"
        
        # 3. ìœ ì‚¬ í‚¤ì›Œë“œ ë§¤ì¹­
        if not match_found:
            similar_keywords = find_similar_keywords(keyword, full_text, threshold=0.6)
            if similar_keywords:
                # ìœ ì‚¬ë„ì— ë”°ë¼ ì ìˆ˜ ì¡°ì •
                similarity_score = 0.08  # ìœ ì‚¬ ë§¤ì¹­ì€ ì•½ê°„ ë‚®ì€ ë³´ë„ˆìŠ¤
                keyword_bonus += weight * similarity_score
                if keyword not in matched_keywords:
                    matched_keywords.append(keyword)
                match_type = f"ìœ ì‚¬({', '.join(similar_keywords[:2])})"
        
        # ë§¤ì¹­ ìƒì„¸ ì •ë³´ ì €ì¥
        if match_type:
            matched_details[keyword] = match_type
    
    # ìµœì¢… ì ìˆ˜ = ë²¡í„° ìœ ì‚¬ë„ + í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
    final_score = vector_score + keyword_bonus
    
    return final_score, matched_keywords


def retrieve_similar_jobs(resume: Dict, slots: Dict, top_k: int = 10) -> List[Dict]:
    """ì´ë ¥ì„œì™€ ì±—ë´‡ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬ ê³µê³  ì¶”ì¶œ (Retriever)
    
    ê°œì„ ì‚¬í•­:
    - ë³µí•© í‚¤ì›Œë“œ ë³´ì¡´ (ë¶„ë¦¬í•˜ì§€ ì•ŠìŒ)
    - ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
    - ë²¡í„° ìœ ì‚¬ë„ + í‚¤ì›Œë“œ ë§¤ì¹­ ê²°í•©
    - í´ë°± ë¡œì§ ì œê±° (ë‹¨ì¼ ê²€ìƒ‰ ì „ëµ)
    """
    print("\n" + "="*80)
    print("ğŸ” Retriever ì‹œì‘")
    print("="*80)
    print("ğŸ“ ì…ë ¥ ì •ë³´:")
    print(f"   - ì´ë ¥ì„œ: {resume.get('name', 'N/A') if resume else 'None'}")
    print(f"   - ì±—ë´‡ ì •ë³´: {slots}")
    
    # í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ìµœì‹  ìƒíƒœë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ import
    from .vector_store import search_vector_store, EMBEDDING_MODEL, VECTOR_STORE
    
    if not VECTOR_STORE or not EMBEDDING_MODEL:
        print("âŒ ë²¡í„° ìŠ¤í† ì–´ ë˜ëŠ” ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print(f"   - VECTOR_STORE: {VECTOR_STORE is not None}")
        print(f"   - EMBEDDING_MODEL: {EMBEDDING_MODEL is not None}")
        print("   ğŸ’¡ ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•±ì„ ì¬ì‹œì‘í•˜ê±°ë‚˜ /api/initialize-vector-storeë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.")
        return []
    
    try:
        # ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì¿¼ë¦¬ êµ¬ì„±
        print("\nğŸ“‹ ì¿¼ë¦¬ êµ¬ì„± ì¤‘...")
        query_keywords, keyword_weights = build_weighted_query(resume, slots)
        
        print(f"   - í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸: {query_keywords}")
        print(f"   - í‚¤ì›Œë“œ ê°œìˆ˜: {len(query_keywords)}")
        
        if not query_keywords:
            print("âŒ ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            print(f"   - ì´ë ¥ì„œ ì •ë³´: {bool(resume)}")
            print(f"   - ì±—ë´‡ ì •ë³´: {slots}")
            return []
        
        print(f"\nğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {query_keywords}")
        print(f"ğŸ“Š í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ({len(keyword_weights)}ê°œ):")
        for keyword, weight in sorted(keyword_weights.items(), key=lambda x: x[1], reverse=True):
            print(f"   - '{keyword}': {weight}")
        
        # í†µí•© ê²€ìƒ‰ (í•œ ë²ˆì˜ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬, ì¶©ë¶„íˆ ë§ì€ chunk ê°€ì ¸ì˜¤ê¸°)
        chunks_per_job = 3  # ê° ê³µê³ ì—ì„œ ê°€ì ¸ì˜¬ chunk ê°œìˆ˜
        search_top_k = top_k * chunks_per_job * 2  # ì¶©ë¶„íˆ ë§ì€ chunk ê°€ì ¸ì˜¤ê¸°
        print(f"\nğŸ” ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰ ì‹¤í–‰ (top_k={search_top_k}, ê³µê³ ë‹¹ {chunks_per_job}ê°œ chunk)...")
        search_results = search_vector_store(query_keywords, top_k=search_top_k)
        
        print("\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼:")
        print(f"   - ë°˜í™˜ëœ chunk ìˆ˜: {len(search_results)}ê°œ")
        
        if not search_results:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print(f"   - ê²€ìƒ‰ í‚¤ì›Œë“œ: {query_keywords}")
            print("   ğŸ’¡ ë²¡í„° ìŠ¤í† ì–´ì— ë°ì´í„°ê°€ ìˆëŠ”ì§€, ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ì ì ˆí•œì§€ í™•ì¸í•˜ì„¸ìš”.")
            return []
        
        # job_idë³„ë¡œ chunk ê·¸ë£¹í•‘
        from collections import defaultdict
        job_chunks = defaultdict(list)
        
        for result in search_results:
            metadata = result.get('metadata', {})
            job_id = metadata.get('job_id')
            if job_id is not None:
                job_chunks[job_id].append(result)
        
        print(f"âœ… ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ chunk, {len(job_chunks)}ê°œ ê³µê³ ")
        
        # ê° ê³µê³ ì˜ ì „ì²´ í…ìŠ¤íŠ¸(full_text)ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
        print(f"\nğŸ“Š ê° ê³µê³  ì „ì²´ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸ ì¤‘... ({len(job_chunks)}ê°œ ê³µê³ )")
        print(f"   ê²€ìƒ‰ í‚¤ì›Œë“œ (5ê°œ): {query_keywords}")
        job_scores = {}  # job_id -> (ìµœì¢… ì ìˆ˜, matched_keywords, chunks)
        
        for job_id, chunks in job_chunks.items():
            try:
                # ê³µê³ ì˜ full_text ê°€ì ¸ì˜¤ê¸° (ì²« ë²ˆì§¸ chunkì˜ metadataì—ì„œ)
                if not chunks:
                    continue
                
                first_chunk_metadata = chunks[0].get('metadata', {})
                full_text = first_chunk_metadata.get('full_text', '')
                
                # full_textê°€ ì—†ìœ¼ë©´ êµ¬ì¡°í™”ëœ ì •ë³´ë¡œ êµ¬ì„±
                if not full_text:
                    metadata_text_parts = [
                        first_chunk_metadata.get('title', ''),
                        first_chunk_metadata.get('company', ''),
                        first_chunk_metadata.get('location', ''),
                        first_chunk_metadata.get('job_type', ''),
                        first_chunk_metadata.get('company_size', ''),
                        first_chunk_metadata.get('industry', ''),
                    ]
                    metadata_text = ' '.join([str(part) for part in metadata_text_parts if part])
                    # ëª¨ë“  chunkì˜ document ê²°í•©
                    all_chunk_texts = ' '.join([chunk.get('document', '') for chunk in chunks])
                    full_text = f"{metadata_text} {all_chunk_texts}"
                
                # ê³µê³  ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
                full_text_lower = full_text.lower()
                matched_keywords = []
                matched_count = 0
                
                for keyword in query_keywords:
                    keyword_lower = keyword.lower()
                    # ì •í™•í•œ ë§¤ì¹­ í™•ì¸
                    if keyword_lower in full_text_lower:
                        matched_keywords.append(keyword)
                        matched_count += 1
                    # ê³µë°± ì œê±° í›„ ë§¤ì¹­ í™•ì¸
                    elif keyword_lower.replace(' ', '') in full_text_lower.replace(' ', ''):
                        if keyword not in matched_keywords:
                            matched_keywords.append(keyword)
                            matched_count += 1
                    # ìœ ì‚¬ í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
                    else:
                        similar_keywords = find_similar_keywords(keyword, full_text, threshold=0.6)
                        if similar_keywords and keyword not in matched_keywords:
                            matched_keywords.append(keyword)
                            matched_count += 1
                
                # ê° chunkì˜ ì ìˆ˜ ê³„ì‚° (ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜)
                chunk_scores = []
                for chunk in chunks:
                    distance = chunk.get('distance', 1.0)
                    vector_score = max(0.0, 1.0 - (distance / 2.0))
                    chunk_scores.append({
                        'chunk': chunk,
                        'score': vector_score
                    })
                
                # chunkë“¤ì„ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                chunk_scores.sort(key=lambda x: x['score'], reverse=True)
                
                # ìƒìœ„ Nê°œ chunk ì„ íƒ
                top_chunks = chunk_scores[:chunks_per_job]
                
                # ê³µê³ ì˜ ìµœì¢… ì ìˆ˜ = ìƒìœ„ chunkë“¤ì˜ í‰ê·  ë²¡í„° ì ìˆ˜ + í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
                if top_chunks:
                    avg_vector_score = sum(c['score'] for c in top_chunks) / len(top_chunks)
                    # í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤: ë§¤ì¹­ëœ í‚¤ì›Œë“œ ê°œìˆ˜ì— ë¹„ë¡€
                    keyword_bonus = sum(keyword_weights.get(kw, 1.0) * 0.1 for kw in matched_keywords)
                    final_score = avg_vector_score + keyword_bonus
                else:
                    final_score = 0.0
                
                job_scores[job_id] = {
                    'final_score': final_score,
                    'matched_keywords': matched_keywords,
                    'matched_count': matched_count,  # ë§¤ì¹­ëœ í‚¤ì›Œë“œ ê°œìˆ˜
                    'total_keywords': len(query_keywords),  # ì „ì²´ í‚¤ì›Œë“œ ê°œìˆ˜
                    'chunks': [c['chunk'] for c in top_chunks],
                    'chunk_count': len(top_chunks),
                    'total_chunks': len(chunks)
                }
                
                # ë§¤ì¹­ ê²°ê³¼ ì¶œë ¥ (ê° ê³µê³ ë§ˆë‹¤ ëª‡ ê°œì˜ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ëŠ”ì§€)
                matched_str = ', '.join(matched_keywords) if matched_keywords else '(ì—†ìŒ)'
                print(f"   ê³µê³  {job_id}: {matched_count}/{len(query_keywords)}ê°œ í‚¤ì›Œë“œ ë§¤ì¹­")
                print(f"      â†’ ë§¤ì¹­ëœ í‚¤ì›Œë“œ: {matched_str}")
                
            except Exception as e:
                print(f"   âš ï¸  ê³µê³  {job_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        if not job_scores:
            print("âŒ ì ìˆ˜ ê³„ì‚° í›„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # ìµœì¢… ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_jobs = sorted(job_scores.items(), key=lambda x: x[1]['final_score'], reverse=True)
        
        # ìƒìœ„ ê²°ê³¼ ì¶œë ¥ (ë§¤ì¹­ ê°œìˆ˜ ê¸°ì¤€ìœ¼ë¡œë„ ì •ë ¬ ê°€ëŠ¥)
        print(f"\nğŸ“ˆ ìƒìœ„ {min(5, len(sorted_jobs))}ê°œ ê³µê³  (ì ìˆ˜ ê¸°ì¤€):")
        print("=" * 80)
        for i, (job_id, job_data) in enumerate(sorted_jobs[:5], 1):
            final_score = job_data['final_score']
            matched = job_data['matched_keywords']
            matched_count = job_data['matched_count']
            total_keywords = job_data['total_keywords']
            chunk_count = job_data['chunk_count']
            
            print(f"{i}. ê³µê³  ID: {job_id}")
            print(f"   ìµœì¢… ì ìˆ˜: {final_score:.3f} (ìƒìœ„ {chunk_count}ê°œ chunk í‰ê· )")
            print(f"   í‚¤ì›Œë“œ ë§¤ì¹­: {matched_count}/{total_keywords}ê°œ")
            if matched:
                print(f"   ë§¤ì¹­ëœ í‚¤ì›Œë“œ: {', '.join(matched)}")
            else:
                print("   ë§¤ì¹­ëœ í‚¤ì›Œë“œ: (ì—†ìŒ)")
            print()
        print("=" * 80)
        
        # ë§¤ì¹­ ê°œìˆ˜ ê¸°ì¤€ ì •ë ¬ (ë¹„êµìš©) - ê³µê³ ë§ˆë‹¤ ëª‡ ê°œì˜ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ëŠ”ì§€ ë¹„êµ
        sorted_by_match = sorted(job_scores.items(), key=lambda x: (x[1]['matched_count'], x[1]['final_score']), reverse=True)
        print(f"\nğŸ“Š ë§¤ì¹­ ê°œìˆ˜ ê¸°ì¤€ ìƒìœ„ {min(5, len(sorted_by_match))}ê°œ ê³µê³  (í‚¤ì›Œë“œ í¬í•¨ ê°œìˆ˜ ë¹„êµ):")
        print("=" * 80)
        for i, (job_id, job_data) in enumerate(sorted_by_match[:5], 1):
            matched_count = job_data['matched_count']
            total_keywords = job_data['total_keywords']
            matched = job_data['matched_keywords']
            final_score = job_data['final_score']
            
            print(f"{i}. ê³µê³  ID: {job_id}")
            print(f"   í‚¤ì›Œë“œ ë§¤ì¹­: {matched_count}/{total_keywords}ê°œ í¬í•¨")
            if matched:
                print(f"   ë§¤ì¹­ëœ í‚¤ì›Œë“œ: {', '.join(matched)}")
            else:
                print("   ë§¤ì¹­ëœ í‚¤ì›Œë“œ: (ì—†ìŒ)")
            print(f"   ìµœì¢… ì ìˆ˜: {final_score:.3f}")
            print()
        print("=" * 80)
        
        # ì „ì²´ ê³µê³ ë³„ ë§¤ì¹­ ê°œìˆ˜ ìš”ì•½
        print("\nğŸ“‹ ì „ì²´ ê³µê³ ë³„ í‚¤ì›Œë“œ ë§¤ì¹­ ìš”ì•½:")
        print("=" * 80)
        match_count_distribution = {}
        for job_id, job_data in job_scores.items():
            count = job_data['matched_count']
            match_count_distribution[count] = match_count_distribution.get(count, 0) + 1
        
        for count in sorted(match_count_distribution.keys(), reverse=True):
            job_num = match_count_distribution[count]
            print(f"   {count}/{total_keywords}ê°œ ë§¤ì¹­: {job_num}ê°œ ê³µê³ ")
        print("=" * 80)
        
        # ìƒìœ„ top_kê°œ ê³µê³  ë°˜í™˜ (ê° ê³µê³ ì˜ ìƒìœ„ chunkë“¤ í¬í•¨)
        final_results = []
        for job_id, job_data in sorted_jobs[:top_k]:
            # ëŒ€í‘œ chunk ì„ íƒ (ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ chunk)
            if job_data['chunks']:
                representative_chunk = job_data['chunks'][0].copy()
                # ë©”íƒ€ë°ì´í„°ì— í†µí•© ì •ë³´ ì¶”ê°€
                representative_chunk['metadata']['final_score'] = job_data['final_score']
                representative_chunk['metadata']['matched_keywords'] = job_data['matched_keywords']
                representative_chunk['metadata']['match_count'] = job_data['matched_count']  # ë§¤ì¹­ëœ í‚¤ì›Œë“œ ê°œìˆ˜
                representative_chunk['metadata']['total_keywords'] = job_data['total_keywords']  # ì „ì²´ í‚¤ì›Œë“œ ê°œìˆ˜
                representative_chunk['metadata']['chunk_count'] = job_data['chunk_count']
                representative_chunk['metadata']['total_chunks'] = job_data['total_chunks']
                final_results.append(representative_chunk)
        
        print(f"\nâœ… Retriever ì™„ë£Œ: {len(final_results)}ê°œ ê³µê³  ë°˜í™˜ (ê° ê³µê³ ë‹¹ í‰ê·  {sum(job_scores[jid]['chunk_count'] for jid, _ in sorted_jobs[:top_k]) / len(final_results) if final_results else 0:.1f}ê°œ chunk)")
        print("="*80 + "\n")
        return final_results
        
    except Exception as e:
        print(f"âŒ Retriever ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return []
